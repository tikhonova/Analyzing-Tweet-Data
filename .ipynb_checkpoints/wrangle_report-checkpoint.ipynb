{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documentation for data wrangling steps: gather, assess, and clean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  1. Gathering data:\n",
    "- [Import libraries](#headin)\n",
    "- [Upload and read the original file](#id2)\n",
    "- [Import tweet image predictions (a TSV file stored on Udacity servers](#id3)\n",
    "- [Gather additional data via Twitter API](#id4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Importing libraries<a name=\"headin\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import tweepy\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Uploading and reading the original file<a name=\"id2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('twitter-archive-enhanced.csv')\n",
    "df.info()\n",
    "df = df_upl.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('twitter-archive-enhanced.csv')\n",
    "df.info()\n",
    "df = df_upl.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting a list of tweet ids to use with API later down the line\n",
    "tweet_ids = df.tweet_id.tolist()\n",
    "tweet_ids [:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Importing tweet image predictions <a name=\"id3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv\" \n",
    "req = requests.get(url)\n",
    "url_content = req.content\n",
    "tsv_file = open('image_predictions.tsv', 'wb')\n",
    "tsv_file.write(url_content)\n",
    "tsv_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsv_upload = pd.read_csv('image_predictions.tsv', sep='\\t')\n",
    "tsv_upload.head()\n",
    "tsv = tsv_upload.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Gathering additional data via Twitter API <a name=\"id4\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create an API object to gather Twitter data\n",
    "consumer_key = 'HIDDEN'\n",
    "consumer_secret = 'HIDDEN'\n",
    "access_token = 'HIDDEN'\n",
    "token_secret = 'HIDDEN'\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, token_secret)\n",
    "api = tweepy.API(auth, parser = tweepy.parsers.JSONParser(), wait_on_rate_limit = True, wait_on_rate_limit_notify = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing credentials\n",
    "try:\n",
    "    api.verify_credentials()\n",
    "    print(\"Authentication OK\")\n",
    "except:\n",
    "    print(\"Error during authentication\")\n",
    "    \n",
    "#Authentication OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "error_dict = {}\n",
    "result=[]\n",
    "import json\n",
    "\n",
    "with open('tweet_json.txt', 'w') as f:\n",
    "    for tw in tweet_ids:\n",
    "        index +=1\n",
    "        print(str(index) + ':' + str(tw))\n",
    "        try:\n",
    "            result = api.get_status(tw, tweet_mode = 'extended')\n",
    "            print('Success')\n",
    "            f.write(json.dumps(result) + '\\n')\n",
    "        except tweepy.TweepError as e:\n",
    "            print('fail')\n",
    "            error_dict[tw] = e\n",
    "            pass\n",
    "print(error_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  2. Assessing data:\n",
    "\n",
    "2.1. Read json.txt. file:\n",
    "   \n",
    "     with open('tweet_json.txt', 'rb') as file:\n",
    "        data = file.readlines()\n",
    "        \n",
    "2.2. Load into the dataframe:\n",
    "\n",
    "    with open('tweet_json.txt', encoding=\"utf8\") as f:\n",
    "        data = f.readlines()\n",
    "        data = [json.loads(line) for line in data]\n",
    "    dftw = pd.DataFrame(data)\n",
    "    \n",
    "2.3. Display all columns in the dataframe:\n",
    " \n",
    "    from IPython.display import display\n",
    "    pd.options.display.max_columns = None\n",
    "    display(dftw)\n",
    "\n",
    "2.4. Get basic info about the data:\n",
    "\n",
    "    dftw.info()\n",
    "    dftw.describe()\n",
    "    display(dftw.user[0])\n",
    "    dftw.id.head()\n",
    "\n",
    "2.5. Check for duplicates:\n",
    "\n",
    "    !pip install hashable_df\n",
    "    from hashable_df import hashable_df\n",
    "    hashable_df(df2).duplicated().sum()\n",
    "    \n",
    "2.6. Check for null values\n",
    "\n",
    "    df2.isnull().sum().sum()\n",
    "    \n",
    "2.7. Show a sample of NaN rows\n",
    "\n",
    "    df2.loc[df.isnull().any(axis=1)].head()\n",
    "    \n",
    "2.8. Confirm that the # of tweets pulled via API matches the total from the original file:\n",
    "\n",
    "    api_tweets = df2.tweet_id.tolist()\n",
    "    matching_tweets = set(tweet_ids).intersection(api_tweets)\n",
    "    len(matching_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  3. Cleaning data\n",
    "\n",
    "### Tidiness -- Define:\n",
    "\n",
    "1. Dog rating variable is split in two columns: combine the two to make a unified column\n",
    "2. Irrelevant data is obstructing the view: drop unneeded columns and rows\n",
    "3. Relevant data is stored in separate tables: combine into one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tidiness -- Code & Test:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Make a copy of the dataset first:\n",
    "        \n",
    "    df2=dftw.copy()\n",
    "    \n",
    "#Dog Rating numerator & denominator need to be combined into a single column\n",
    "\n",
    "    whole['rating_numerator'] = whole['rating_numerator'].astype(str)\n",
    "    whole['rating_denominator'] = whole['rating_denominator'].astype(str)\n",
    "    whole['rating'] = whole['rating_numerator'].str.cat(whole['rating_denominator'],sep=\"/\")\n",
    "    whole.drop(['rating_denominator','rating_numerator'],axis=1,inplace=True)\n",
    "    whole = whole.reindex(columns = whole.columns)\n",
    "\n",
    "#Drop some unneeded columns, example below:\n",
    "\n",
    "    df2.drop(['id_str',\n",
    "    'in_reply_to_status_id_str',\n",
    "    'in_reply_to_user_id_str',\n",
    "    'quoted_status_id_str','place'],axis=1,inplace=True)\n",
    "    \n",
    "#Add prefix and merge the datasets:\n",
    "\n",
    "    df2 = df2.add_prefix('api_')\n",
    "    tsv = tsv.add_prefix('img_')\n",
    "    df_new = df.merge(df2,on=['tweet_id','source'], how='left')\n",
    "    whole = df_new.merge(tsv,on=['tweet_id'], how='left')\n",
    "\n",
    "#Inspect columns in the new dataset:\n",
    "\n",
    "    whole.columns.sort_values()\n",
    "    whole.sample()\n",
    "    \n",
    "#Return only rows containing images, and then reducing to those that have their API additional info\n",
    "\n",
    "    whole=whole.query(\"img_jpg_url == img_jpg_url\")\n",
    "    whole=whole.query(\"api_favorited==api_favorited\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality -- Define:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \"Timestamp\" column variables are stored as string and should be converted to date/time\n",
    "- 'id' needs to be renamed as 'tweet_id'\n",
    "- 'api_source' needs to be renamed as 'source'\n",
    "- \"API_Entitiies\" columns contains a series of dictionatiries (variables) that need to be split into separate columns\n",
    "- Dog ratings are stored as string and need to be converted to float, so we can query them later\n",
    "- Erroneous data need to be fixed, e.g. dog ratings should be over 1 (12/10, 11/10), those that are less than 1 (9/10, 8/10) are incorrect as they do not meet the standard of WeRateDogs\n",
    "- \"api_retweet_count\" and \"api_favorite_count\" column variables are stored as floats and need to be converted to integers\n",
    "- \"api_user\" has multiple variables stored in one column: split into multiple those that are needed and drop the rest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality -- Code & Test:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Timestamp is an object --> need to be date/time\n",
    "\n",
    "    whole['timestamp'] = pd.to_datetime(whole['timestamp'])\n",
    "    \n",
    "    \n",
    "#Performing column renaming, example below:\n",
    "\n",
    "    df2.rename(columns={'id':'tweet_id'},inplace=True)\n",
    "    df2.rename(columns={'api_source':'source'},inplace=True)\n",
    "\n",
    "\n",
    "#Splitting API_Entitiies series of dictionatiries into separate columns; pulling hashtag values into a column.\n",
    "\n",
    "    whole = pd.concat([whole.drop(['api_entities'], axis=1),\n",
    "    whole['api_entities'].apply(pd.Series)], axis=1)\n",
    "\n",
    "    whole[['hashtag','indices']] = pd.DataFrame(whole.hashtags.tolist(), \n",
    "    index= whole.index)\n",
    "    whole.drop(['hashtags','indices'],axis=1,inplace=True)\n",
    "\n",
    "    whole = pd.concat([whole.drop(['hashtag'], axis=1), \n",
    "    whole['hashtag'].apply(pd.Series)], axis=1)\n",
    "\n",
    "#Fixing dog ratings\n",
    "\n",
    "    #how many ratings cotain the \"/10\" denominator?\n",
    "    whole[whole['rating'].str.contains(\"/10\")].tweet_id.count()\n",
    "    #some of thsoe are still inaccurate, e.g. 8/10, 2/10\n",
    "\n",
    "    #converting string ratings to floats so we can query them later\n",
    "    whole['float_rating'] = whole.rating.fillna(1000).apply(pd.eval)\n",
    "\n",
    "    incorrect_ratings = whole.query('float_rating < 1')\n",
    "\n",
    "    whole = whole[whole.float_rating > 1]\n",
    "    \n",
    "    \n",
    "#Converting count variables into integers:\n",
    "\n",
    "    whole.api_retweet_count=whole.api_retweet_count.astype(int)\n",
    "    whole.api_favorite_count=whole.api_favorite_count.astype(int)\n",
    "\n",
    "\n",
    "#Extracting followers' count from api_user into a separate column\n",
    "\n",
    "    followers_count = [d.get('followers_count') for d in user_values]\n",
    "\n",
    "    unique_list = (list(set(followers_count)))\n",
    "    for x in unique_list: \n",
    "            print (x)\n",
    "\n",
    "    whole['followers_count'] = pd.DataFrame([x for x \n",
    "    in whole['api_user']])['followers_count']\n",
    "    \n",
    "\n",
    "#Extracting statuses count and frineds count value into separate columns as well:\n",
    "\n",
    "    whole['statuses_count'] = pd.DataFrame([x for x in whole['api_user']])['statuses_count']\n",
    "    whole['friends_count'] = pd.DataFrame([x for x in whole['api_user']])['friends_count']\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
